{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacking Sift\n",
    "## I. INTRODUCTION\n",
    "---\n",
    "The goal of this codelab is to get you into the details of the Scale Invariant Features Transform (SIFT). SIFT is an algorithm in computer vision to detect and to describe local features in images. Its applications include objects recognition, robotic mapping and navigation, image stitching, 3D modeling, video tracking and others.\n",
    "The lab session has the aim first, to introduce you, both on the theorical and practical aspects of the tool, and then to let you apply the algorithm in a real computer vision applications. After a brief introduction on how SIFT works and on how to apply them, the lab will guide you on building an image stitching pipeline for creating panorama images. The lab has been implemented in python language, using OpenCV and numpy libraries.\n",
    "The image stitching suggested is composed by a cascade of a features detector module (sift), a features matching module (flann) and an inlier pairs matches estimation for selecting the best matches (ransac). The modules briefly anticipated will be explained thoroughly later on. The approach to image stitching that has been proposed as example can be slower and less accurate compared to some other approaches that do not use features, however the results obtained with this approach are much more robust to noise, lighting gradients and a priori knowledge is not asked about the relative position between images.\n",
    "Alternatively to SIFT, other features modules, such as surf, orb, brisk, kaze and so on, can be applied in the implementation of the pipeline. Of course, their parameterizations changes completely.\n",
    "\n",
    "## II. SIFT- SCALE INVARIANT FEATURES TRANSFORM \n",
    "---\n",
    "### A. Overview\n",
    "A SIFT feature is a salient keypoint that corresponds to an image region, that has associated a descriptor. SIFT computation is commonly divided in two different steps:\n",
    "* detector;\n",
    "* descriptor.\n",
    "At the end of the detection phase, for each features detected, the SIFT algorithm has established:\n",
    "* keypoint spatial coordinates (x,y); \n",
    "* keypoint scale;\n",
    "* keypoint dominant orientation.\n",
    "After the detection step, the SIFT descriptor step computes a distinctive fingerprint for each feature. The description obtained is designed to be invariant to scale and rotation. Moreover, the algorithm offers decent robustness to noise, illumination gradients and affine transformations. \n",
    "\n",
    "### B. SIFT detector\n",
    "SIFT detector outcomes are SIFT keypoints. A keypoint is entirely described by 4 values, its x and y keypoint center coordinates, its scale, that is indicated by the radius of the keypoint region, its orientation, that is an angle expressed in radiant that will be explained later on.\n",
    "![Representation of a detected SIFT keypoint. The four parameters are underlined: spatial coordinates, radius of the region, that determines the scale, and orientation](latex_lab_tutorial_sources/imgs/detected_sift.jpg \"Representation of a detected SIFT keypoint. The four parameters are underlined: spatial coordinates, radius of the region, that determines the scale, and orientation\")\n",
    "SIFT detection starts by building a pyramidal scale space of Difference of Gaussians. For explaining the concept of DoG pyramid, the octave will be first introduced. An octave is a n ∗ n box of pixels, with n typically equals to 16, an image is divided in a grid of non-overlapping octaves. A square composed by 4 octaves in an image is mapped in one octave in the layer above, if there are any, and it corresponds to 16 octaves in the layer below, if there are any.\n",
    "![ Representation of an octave and its relative part in its correspondent downsized. A 16x16 pixels octave in an upper layer is mapped to a 4x4 pixels portion of the correspondent octave downstream.](latex_lab_tutorial_sources/imgs/octave_downsize.jpg \"Representation of an octave and its relative part in its correspondent downsized. A 16x16 pixels octave in an upper layer is mapped to a 4x4 pixels portion of the correspondent octave downstream.\")\n",
    "\n",
    "A pyramidal scale space is built by starting from the layer with the highest number of octaves, at the bottom of the pyramid, that may be composed by an octaves grid of the size of original image, or an octaves grid of twice as much the size of the original image (as it happens in Lowe’s SIFT). \n",
    "![Representation of a scale space DoG pyramid. There are just 2 octaves. For each octave, there are 4 scales. Each DoG in an octave layer is computed with a decreasing gaussian kernel size. In SIFT Lowe’s paper there are 3 scales for each octave layer.\n",
    "](latex_lab_tutorial_sources/imgs/dog_pyramid.jpg \"Representation of a scale space DoG pyramid. There are just 2 octaves. For each octave, there are 4 scales. Each DoG in an octave layer is computed with a decreasing gaussian kernel size. In SIFT Lowe’s paper there are 3 scales for each octave layer.\")\n",
    "\n",
    "Then, the layers above are iteratively built by successively downsizing the octave grid at the current scale by a factor of 2. It results that a DoG pyramid is composed by $log_2 N$, or $log_2 (N) + 1$, octaves layers, depending on the starting grid size, where N is the minimum between the width and the height of the image. While downsizing the octaves in the pyramid, the gaussian kernel used to compute the DoG is sized up. For instance a downsize of a factor 2 of the octaves, is performed together at an upsizing of a factor of 2 of the gaussian kernel used for the DoG. Nevertheless, one octaves layer may correspond to several DoG of the image, in fact in Lowe’s paper a octaves layer contains 3 different DoG performed with 3 different increasing scales of gaussian kernel (Fig. above). For instance, the first octave layer, supposing that has an octaves grid of the size of the original image, may have associated 3 DoGs, the first obtained with gaussian kernel with sigma 1, the second obtained with gaussian kernel with sigma 1.3, the third obtained with gaussian kernel with sigma 1.6. Then, the sigma of the gaussian kernels in the octave layers above may be 2, 2.3 and 2.6.\n",
    "\n",
    "In particular, sigma of the gaussian kernels used for computing the DoG pyramid are sampled as:\n",
    "$$\\sigma = \\sigma_0*2^{o+s/S},    s = 0,...,S-1,    o=o_{min}, ..., o_{min} + O -1$$\n",
    "\n",
    "where $\\sigma_0=1.6$, $o_{min}$ is the first octave index, O the number of octaves and S the number of scales per octaves.\n",
    "  \n",
    "The gaussian smoothed image at a certain scale determined by $\\sigma$ is computed by:\n",
    "$$I_{\\sigma} = g_{\\sigma} * I$$\n",
    "where $I$ is the original image and $g_{\\sigma}$ is the gaussian kernel for that scale.\n",
    "\n",
    "The DoG for a certain scale s and certain octave o is:\n",
    "$$DoG_{\\sigma(o,s)} = I_{\\sigma(o,s+1)} - I_{\\sigma(o,s)}$$\n",
    "where $o=o_{min},...,o_{min}+O-1$ and $s = 0,...,S-1$\n",
    "Then, keypoints are identified as local maxima/minima of the DoG images across the scale. Maxima/min-\n",
    "ima are obtained comparing each pixel with its 8 neighbours, if the pixel is a maximum/minimum within that neighborhood, than it is a SIFT keypoint candidate. For increasing the accuracy of the maxima/minima locations, the DoG scale-space function is interpolated with finer resolution by quadratic Taylor expansion, so that sub-pixel accuracy can be attained.\n",
    "Local maxima/minima with low contrast are unstable and may be source of error. In order to discard such candidate features, only the candidates with value of second order of the Taylor expansion greater than a certain threshold are kept, while the others are discard. That threshold in the Lowe paper is 0.03, however it should be tuned according to the characteristics of the images that are taken in analysis.\n",
    "DoG will have strong responses along edges, however candidates features detected over edges are unstable since they have poorly determined spatial location. To increase stability, kepoint candidates with poorly determined locations with high edge response have to be discarded. In order to understand whether a keypoint is on a blob or on an edge, its principal curvature is evaluated. In differential geometry, the two principal curvatures at a given point of a surface are the eigenvalues of the shape operator at that point. They measure how the surface bends by different amounts in different directions at that point. If the keypoint candidate has a principal curvature with the 2 eigenvalues that are strongly different, it is located on an edge, otherwise it is located on a blob, thus is more stable. It follows a score that is related to the ratio of the 2 eigenvalues:\n",
    "\n",
    "$$R = \\frac{(tr D(x,y,\\sigma)^2}{det D(x,y,\\sigma)} = \\frac{(r+1)^2}{r} ,\n",
    "D = \\begin{bmatrix}\n",
    "\\frac{\\partial^2{DoG}}{\\partial^2{x^2}} & \\frac{\\partial^2{DoG}}{\\partial{x}\\partial{y}} \\\\\n",
    "\\frac{\\partial^2{DoG}}{\\partial{x}\\partial{y}} & \\frac{\\partial^2{DoG}}{\\partial^2{x^2}} \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where\n",
    "$r = \\frac{\\alpha}{\\beta}$\n",
    "with $\\alpha$ and $\\beta$ the 2 eigenvalues.\n",
    "This score has minimum value 4 when the 2 eigenvalues are equal, and it increases whether their ratio increases. SIFT discards all the candidate keypoints that have an edge score greater than a certain threshold, that in Lowe's paper is 10.\n",
    "\n",
    "Then, for each keypoint, at the scale that it has been detected, it is precomputed the magnitude and the phase of its gradient over all its extension, that is given by its radius. Thus, a keypoint orientation histogram of 36 channels is built from the gradient phases. Each bin has a resolution of 10°. The histogram is then smoothed. The maximum of the histogram is elected as the main orientation of the keypoint. If there are other orientations that are more than the 80\\% respect with the maximum, new keypoints with same spatial coordinates and radius but with the secondary orientations as main orientations are added.\n",
    "\n",
    "### C. SIFT descriptor\n",
    "Once that keypoints are detected, SIFT transform is designed to provide a description of such features as well. The feature description has to be highly distinctive, so that it can reduce the probability of features mismatch. However, descriptors have to be robust to changes in illumination, noise and 3D viewpoints. The principle adopted for features descriptor is really similar to the one adopted with HOG, Histogram of Oriented Gradients. For each keypoint detected, a neighborhood of 16x16 pixels is taken into account. This neighborhood is splitted in 4x4 cells, each cell composed by 4x4 pixels. For each cell, a histogram of the oriented gradient relatively to the main orientation detected for that keypoint is computed, in order to attain invariance to rotation. A histogram of oriented gradient is a histogram of the phase of gradients for that cell. The histogram has 8 bins, so each bin has a resolution of 40. Since there are 4x4 cells for each neighborhood relatively to each feature, a 4x4x8 histogram is obtained, for a total of 128 for each features.\n",
    "![Representation of a SIFT features descriptor. At the left, the keypoint 16x16 pixels neighborhood containing the values of the oriented gradient is divided into 4 cells. At the right, for each one of the four cells, there is a radial representation of the histogram of gradients.](latex_lab_tutorial_sources/imgs/sift_descriptors.jpeg \"Representation of a SIFT features descriptor. At the left, the keypoint 16x16 pixels neighborhood containing the values of the oriented gradient is divided into 4 cells. At the right, for each one of the four cells, there is a radial representation of the histogram of gradients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Getting things done\n",
    "The Python/OpenCV code for detecting and describing SIFT features on an image, that you can find in the *get_homographyfunction* in *image_alignment.py* , is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "def getsize(img):\n",
    "    h, w = img.shape[:2]\n",
    "    return w, h\n",
    "\n",
    "class image_alignment_sift(object):\n",
    "\n",
    "    SIFT = 0\n",
    "    SURF = 1\n",
    "    ORB = 2     #should be used just with BF MATCHER (uint8 descriptor)\n",
    "    BRISK = 3\n",
    "    \n",
    "    FLANN = 0\n",
    "    BFMATCHER = 1\n",
    "\n",
    "    \n",
    "    def __init__(self, feat_type, matcher_type=FLANN, params = None):\n",
    "        self.detector, norm = self.features_detector(feat_type=feat_type, params = params)\n",
    "        self.matcher = self.features_matcher(matcher_type=matcher_type, norm=norm)\n",
    "    \n",
    "    \n",
    "    def features_detector(self, feat_type = SIFT, params = None):\n",
    "        \n",
    "        assert feat_type == self.SIFT or feat_type == self.SURF or \\\n",
    "            feat_type == self.ORB or feat_type == self.BRISK\n",
    "        \n",
    "        if feat_type == self.SIFT:\n",
    "            \n",
    "            if params is None:\n",
    "                nfeatures = 0\n",
    "                nOctaveLayers = 3\n",
    "                contrastThreshold = 0.04\n",
    "                edgeThreshold=10\n",
    "                sigma=1.6\n",
    "            else:\n",
    "                nfeatures = params[\"nfeatures\"]\n",
    "                nOctaveLayers = params[\"nOctaveLayers\"]\n",
    "                contrastThreshold = params[\"contrastThreshold\"]\n",
    "                edgeThreshold = params[\"edgeThreshold\"]\n",
    "                sigma = params[\"sigma\"]\n",
    "            \n",
    "            detector = cv2.SIFT(nfeatures=0, \n",
    "                                nOctaveLayers=3, contrastThreshold=0.04, \n",
    "                                edgeThreshold=10, sigma=1.6)\n",
    "            norm = cv2.NORM_L2\n",
    "        elif feat_type == self.SURF:\n",
    "            \n",
    "            if params is None:\n",
    "                hessianThreshold = 3000\n",
    "                nOctaves = 1\n",
    "                nOctaveLayers = 1\n",
    "                upright = True\n",
    "                extended = False\n",
    "            else:\n",
    "                hessianThreshold = params[\"hessianThreshold\"]\n",
    "                nOctaves = params[\"nOctaves\"]\n",
    "                nOctaveLayers = params[\"nOctaveLayers\"]\n",
    "                upright = params[\"upright\"]\n",
    "                extended = params[\"extended\"]\n",
    "                \n",
    "            detector = cv2.SURF(hessianThreshold = hessianThreshold, \n",
    "                                nOctaves = nOctaves, \n",
    "                                nOctaveLayers = nOctaveLayers, \n",
    "                                upright = upright, \n",
    "                                extended = extended)\n",
    "            norm = cv2.NORM_L2\n",
    "            \n",
    "        elif feat_type == self.ORB:\n",
    "\n",
    "            detector = cv2.ORB(nfeatures=8000, scaleFactor=1.1, nlevels=8, edgeThreshold=10, firstLevel=0, WTA_K=2, patchSize=10)\n",
    "            norm = cv2.NORM_HAMMING\n",
    "        elif feat_type == self.BRISK:\n",
    "            detector = cv2.BRISK()\n",
    "            norm = cv2.NORM_HAMMING\n",
    "       \n",
    "        return detector, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have read carefully the theorical introduction on Scale Invariant Features Transform, these lines of code will look trivial and without mistery. The first line creates a SIFT object detector and descriptor. The second line detects and describes the keypoints. By using the parameters in input at the features detector constructor, it is possible to tune the desired results. If nfeatures is equal to zero, the number of features is determined according to the other parameters, otherwise it is forced to the value specified. The parameter contrastThreshold is the threshold to estabilish whether a feature has low contrast or not. The higher its value, the stronger has to be a feature to be detected. The edge threshold determines whether a keypoint has to be discarded or not depending its principal curvature. Its minimum value is 4, and it allows only perfect blobs. A value of 10 is a good compromise between features stability and detector sensitivity. The sigma parameter determines the sigma of gaussian kernel used to smooth in the scale space. The variable kp1 is a list of the detected keypoints, that includes their spatial coordinates, their radius and their orientation. The variable desc1 indicates the descriptors relative to those points.\n",
    "By checking OpenCV documentation and tutorials online, write the code necessary to visualize de- tected Sift. The code has to be inserted in the get_homography method of the image_alignment class in image_alignment.py module.\n",
    "What do you expect when the contrast threshold is increased or decreased? What do you expect when the edge threshold is increased or decreased? Try to answer by the theorical understading that you have on SIFT, and later on, by observing the detected features, try experimentally different values and observe the results that you obtain. Was your guess confirmed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. FLANN - FAST LINEAR APPROXIMATED NEAREST NEIGHBOURS \n",
    "---\n",
    "### A. Features pairing\n",
    "Object instances matching is a typical application where tools like SIFT are applied. It consists of matching the same object presents in two different images. The object instances may be rotated, they may have perspective 3D view point distortions or they may be affected by noise. Instances matching with SIFT requires that features of the instances of the same object are matched. Image stitching for building a panorama pictures requires this matching step as well. In fact, features belonging to objects that are shared between the different perspectives are used to perform the alignment to build the panorama. Those features are paired by means of their descriptors. The challenge to pair high dimensional descriptors, as sift descriptors are, it is called to find the nearest neighbours. Exact nearest neighbours do not scale well with the increase of dimensionality of the descriptors and numbers of features. So, an approximate approach is chosen for trading off accuracy while matching faster.\n",
    "FLANN library selects the best performing approximate nearest neighbours algorithm for the dataset. The algorithm selection is performed by cross-validation on a small chunk of the dataset where to perform nearest neighbors search. FLANN features matching is not robust by itself. Several pairs are wrong, since matching is approximate and features descriptors may fail in being distinctive enough. Wrong pairs may become an issue that may compromise image stitching process, if not addressed. A method for dealing with erroneous matching will be explained later on.\n",
    "\n",
    "### B. Getting things done\n",
    "The Python/OpenCV code to implement flann, in the method *filter_matches* of *image_alignment* class in *image_alignment.py* , follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FLANN INDEX KDTREE = 1 # opencv bug : flann enums are missing\n",
    "norm = cv2 .NORM L2 # with different norms parameters may change\n",
    "flann params = dict ( algorithm = FLANN INDEX KDTREE, trees = 5)\n",
    "matcher = cv2 . FlannBasedMatcher ( flann params , {})\n",
    "raw matches = matcher.knnMatch(desc1 , trainDescriptors = desc2 , k = 2)\n",
    "p1, p2, kp pairs = filter matches(kp1, kp2, raw matches)\n",
    "\"\"\"\n",
    "\n",
    "class image_alignment_matcher(image_alignment_sift):\n",
    "\n",
    "    def features_matcher(self, matcher_type = image_alignment_sift.FLANN , norm = cv2.NORM_L2 ):\n",
    "        \n",
    "        FLANN_INDEX_KDTREE = 1  # opencv bug: flann enums are missing\n",
    "        FLANN_INDEX_LSH    = 6\n",
    "        \n",
    "        if matcher_type==self.FLANN:\n",
    "            if norm == cv2.NORM_L2:\n",
    "                flann_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "            else:\n",
    "                flann_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                               table_number = 6, # 12\n",
    "                               key_size = 12,     # 20\n",
    "                               multi_probe_level = 1) #2\n",
    "            matcher = cv2.FlannBasedMatcher(flann_params, {})  # bug : need to pass empty dict (#1329)\n",
    "        else:\n",
    "            matcher = cv2.BFMatcher(norm)\n",
    "        \n",
    "        return matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 4 lines initialize the FLANN matcher. The variables kp1 and desc1 are the features detected in the first image and their relative descriptors. The variables kp2 and desc2 are the features in the second image and their relative descriptors. The knnMatch method of the matcher object find the best pair of neighbours and the second best matching pair, since k is equal to 2. The method ”filter matches” is supposed to discard weak pairs. In fact, if the best matching pair distance is not way smaller than the second best matching pair distance, probably the matched features are not so distinctive, and so that matching should not be considered reliable and it should be discarded. Considering that ”raw matches” is a python list, and its elements are tuple containing the best match and the second best match, the code for filtering the matches may contain something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class image_alignment_filtering(image_alignment_matcher):\n",
    "\n",
    "    def filter_matches(self, kp1, kp2, matches, ratio = 0.75):\n",
    "        \n",
    "        mkp1, mkp2 = [], []\n",
    "        \n",
    "        for m in matches:\n",
    "            \n",
    "            if len(m) == 2 and m[0].distance < m[1].distance * ratio:\n",
    "                \"\"\"\n",
    "                matches is a list. m, an item of matches, a list as well,\n",
    "                containing as first item the best match, encoded as a DMatch, \n",
    "                and the second item, the second best match, encoded as a DMatch.\n",
    "                DMatch is an object that has as attributes, queryIdx and trainIdx\n",
    "                that indicates the indeces of the matched points from kp1 and kp2.\n",
    "                Creates two lists, mpk1 and mpk2 that contains sorted kp1 points \n",
    "                and kp2 points, such that are mantained the same indeces of \n",
    "                the matched points.\n",
    "                \"\"\"                \n",
    "\n",
    "                #TODO\n",
    "\n",
    "                #Remove the next line when the exercise on sorting matched keypoints is done.\n",
    "                raise NotImplementedError(\"You should implement yourself the code!\")\n",
    "                \n",
    "        p1 = np.float32([kp.pt for kp in mkp1])\n",
    "        p2 = np.float32([kp.pt for kp in mkp2])\n",
    "        kp_pairs = zip(mkp1, mkp2)\n",
    "        \n",
    "        return p1, p2, kp_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your self the missing part of the method. What do you expect to obtain while varying the variable ratio? When you will be done in implementing your alignment pipeline, try different values of ratio and look at the differences.Think about to use the explore matching function provided to you in the annex at the end of this lab track for visualizing differences in the obtained results.\n",
    "![Pair matching of two view of New York skyline.](latex_lab_tutorial_sources/imgs/correspondences_rockfeller_landscape.png \"Pair matching of two view of New York skyline.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. RANSAC - RANDOM SAMPLE CONSENSUS ALGORITHM \n",
    "---\n",
    "### A. Homography design with RANSAC\n",
    "Some features pairs are correct, while some others are completely wrong. If the image homograhy designed to perform overlapping is calculated by minimizing the linear projection error that takes into account also the incorrect pairs, the final result will be affected by a displacement that would make the alignment unpleasant for the observers. In order to have acceptable panorama pictures, such an error cannot be accepted, since even really small displacement results are really tedious to sight. So, instead of linear projection, RANSAC is applied. RANSAC is an algorithm that selects a subset of inliers pairs matching, while discarding outliers matching. Inliers are pairs that are ”coherent” with the homography for overlapping, while outliers do not fit into it. Consider a dataset of N features pairs, RANSAC select a smaller subset of M features pairs and calculate the homography model according to them. Just 4 matches are necessary to compute a model of the homography. After calculated the homography, it establishes the number of features pairs that are closer than a certain tolerance, the inliers. If the number of inliers is bigger than a number K, it found a suitable solution, otherwise it will try a new solution selecting a new subset of M samples and repeating the previous steps. In this way, outliers are discarded and only best matches are kept to perform the overlapping. This method relies on the assumtion that is highly likely that there are at least K good pairs of matches, the so called inliers.\n",
    "\n",
    "\n",
    "### B. Getting things done\n",
    "It follows the code for ransac and homography computation in python and OpenCV, that can be found in the method get_method of the class image_alignment in the module image_alignment.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class image_alignment(image_alignment_filtering):\n",
    "    def __init__(self, feat_type, matcher_type=image_alignment_sift.FLANN, params = None):\n",
    "        image_alignment_sift.__init__(self, feat_type, matcher_type=image_alignment_sift.FLANN, params = None)\n",
    "\n",
    "    def get_homography(self, img1, img2, mask1=None, mask2=None):\n",
    "                        \n",
    "        kp1, desc1 = self.detector.detectAndCompute(img1, None)\n",
    "        kp2, desc2 = self.detector.detectAndCompute(img2, None)\n",
    "        \n",
    "        \"\"\"Visualize here detected keypoints\n",
    "\n",
    "        TODO\n",
    "\n",
    "        \"\"\"\n",
    "        #remove this after having done the exercise on visualizing detected points!!!\n",
    "        raise NotImplementedError(\"You should implement yourself the code!\")\n",
    "\n",
    "        raw_matches = self.matcher.knnMatch(desc1, trainDescriptors = desc2, k = 2)\n",
    "        p1, p2, kp_pairs = self.filter_matches(kp1, kp2, raw_matches)\n",
    "        if len(p1) >= 4:\n",
    "            H, status = cv2.findHomography(p1, p2, cv2.RANSAC, 5.0)\n",
    "            \n",
    "            return H, status\n",
    "        else:\n",
    "            return None, None\n",
    "    \n",
    "    def align(self, img1, img2, mask1=None, mask2=None):\n",
    "                \n",
    "        H, status = self.get_homography(img2, img1, mask2, mask1)\n",
    "            \n",
    "        if H is None:\n",
    "            return None, None, None # img1, img2, H\n",
    "        \n",
    "        img2_aligned = cv2.warpPerspective(img2.Image, H, getsize(img2.Image))\n",
    "        \n",
    "        assert img2_aligned is not None\n",
    "        \n",
    "        return img1, img2_aligned, H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 2 parameters are a list of matched points. The third parameter selects the method to be used to de- sign the homography. The fourth parameter ransacReprojThreshold represents the K introduced previously, it indicates the number of inliers that have to be found. It makes sense to set this parameter somewhere in between 4 and 10. Have fun in experimenting changes in the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. IMAGES BLENDING AND OVERLAPPING\n",
    "---\n",
    "### A. Avoiding abrupt changes in lighting conditions\n",
    "Once that the homography matrix is computed, the homography can be applied to the source image, so that it will overlap to the target image. For doing so, it is possible simply to use the cv2.warpPerspective function in OpenCV library. However, this is not enough in order to obtain good panorama images. In fact there may be strong differences of lights due to the fact that exposure times of the shutters and white balance control of the cameras changes when capturing the same scene by different perspectives. So it is necessary to perform images blending, in order to make smoother the discontinuities between the two overlapped pictures.\n",
    "\n",
    "### B. Getting things done\n",
    "In this paragraph, it is proposed a solution to the blending and overlapping challenge. However, this code does handle only overlapping of horizontal panorama. Feel encouraged to experiment blending and overlapping functions that handle vertical alignment and so on. It follows the proposed code of the functions used to perform the overlapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#given the height and the width of the panorama, and the barrier, that indicates #where there is the discontinuity between the images , this function produce\n",
    "#a smoothed transient in the overlapping .\n",
    "#smoothing window is a parameter that determines the width of the transient #left biased is a flag that determines whether it is masked the left image,\n",
    "#or the right one\n",
    "def blending_mask(height, width, barrier, smoothing_window, left_biased=True):\n",
    "    assert barrier < width\n",
    "    mask = np.zeros((height, width))\n",
    "    offset = int(smoothing_window/2) \n",
    "    if left_biased:\n",
    "        mask[:,barrier-offset:barrier+offset+1]=np.tile(np.linspace(1,0,2*offset+1).T, (height, 1))\n",
    "        mask[:,:barrier-offset] = 1 \n",
    "    else :\n",
    "        mask[:,barrier-offset:barrier+offset+1]=np.tile(np.linspace(0,1,2*offset+1).T, (height, 1)) \n",
    "        mask[: , barrier+offset :] = 1\n",
    "    return cv2.merge([mask, mask, mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you familiar with broadcasting addressing in numpy array? Check it out on numpy documentation. What is the the method linspace of numpy library supposed to do? And the method tile? Check them out as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#this function apply the homography to img2 and it performs the blending while doing so\n",
    "def images_blending (img1 , img2 , width_panorama , height_panorama , H, smoothing_window = 400) :\n",
    "    barrier = img1.shape[1] -int(smoothing_window/2)\n",
    "    panorama1 = np.zeros((height_panorama, width_panorama, 3))\n",
    "    mask1 = blending_mask ( height_panorama , width_panorama , barrier , smoothing_window = smoothing_window ,\n",
    "    left_biased = True)\n",
    "    panorama1[0:img1.shape[0] ,0:img1.shape[1] ,:] = img1 \n",
    "    panorama1 *= mask1\n",
    "    mask2 = blending_mask ( height_panorama , width_panorama , barrier , smoothing_window = smoothing_window , left_biased = False)\n",
    "    panorama2 = cv2.warpPerspective(img2, H, (width_panorama, height_panorama)) * mask2 \n",
    "    return panorama1 + panorama2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those methods can be found in the blending.py module. What is this function supposed to do in detail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#apply the homography for overlapping images, meanwhile performing blending between 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NotImplementedType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-0dfe341aa25c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#sift features computation -> flann (approximate nearest neighbours) -> ransac for outliers removal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_align\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_homography\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m#apply the homography for overlapping images, meanwhile performing blending between\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-0ea14ab3410a>\u001b[0m in \u001b[0;36mget_homography\u001b[0;34m(self, img1, img2, mask1, mask2)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \"\"\"\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#remove this after having done the exercise on visualizing detected points!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You should implement yourself the code!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mraw_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknnMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDescriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NotImplementedType' object is not callable"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "You should implement yourself the code!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-0dfe341aa25c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#sift features computation -> flann (approximate nearest neighbours) -> ransac for outliers removal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_align\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_homography\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m#apply the homography for overlapping images, meanwhile performing blending between\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-804c7d311e83>\u001b[0m in \u001b[0;36mget_homography\u001b[0;34m(self, img1, img2, mask1, mask2)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \"\"\"\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#remove this after having done the exercise on visualizing detected points!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You should implement yourself the code!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mraw_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mknnMatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDescriptors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdesc2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: You should implement yourself the code!"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib notebook\n",
    "%tb\n",
    "\n",
    "fn_base = \"source/data/\"\n",
    "#edit here for trying with different images\n",
    "fn_img1 = \"rockfeller_1.JPG\"\n",
    "fn_img2 = \"rockfeller_2.JPG\"\n",
    "\n",
    "#reading images\n",
    "img1 = cv2.cvtColor(cv2.imread(os.path.join(fn_base, fn_img1)),cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(cv2.imread(os.path.join(fn_base, fn_img2)),cv2.COLOR_BGR2RGB)\n",
    "\n",
    "height_img1 = img1.shape[0]\n",
    "width_img1 = img1.shape[1]\n",
    "nch_img1 = img1.shape[2]\n",
    "#watch out: coordinates in opencv are inverted compared to numpy!\n",
    "img1 = cv2.resize(img1, (width_img1/2, height_img1/2))\n",
    "\n",
    "height_img2 = img2.shape[0]\n",
    "width_img2 = img2.shape[1]\n",
    "nch_img2 = img2.shape[2]\n",
    "img2 = cv2.resize(img2, (width_img2/2, height_img2/2))\n",
    "\n",
    "assert nch_img1 == nch_img2\n",
    "\n",
    "height_panorama = height_img1 / 2\n",
    "width_panorama = width_img1  \n",
    "\n",
    "#sift parameters\n",
    "sift_params = dict()\n",
    "#number of features, if 0, the number will be determined according to the other \n",
    "#parameters\n",
    "sift_params[\"nfeatures\"] = 0\n",
    "sift_params[\"nOctaveLayers\"] = 3\n",
    "#the higher the threshold, the strongest features are kept, while the other\n",
    "#are discarded\n",
    "sift_params[\"contrastThreshold\"] = 0.04\n",
    "sift_params[\"edgeThreshold\"] = 10\n",
    "#sigma of the blurring gaussian kernel used for smoothing when building the\n",
    "#pyramid\n",
    "sift_params[\"sigma\"] = 1.6\n",
    "\n",
    "#creating the instance of the image aligner\n",
    "img_align = image_alignment(feat_type = image_alignment_sift.SIFT, params = sift_params)\n",
    "#designing the homography matrix by detecting matches between features in the\n",
    "#two images\n",
    "#sift features computation -> flann (approximate nearest neighbours) -> ransac for outliers removal\n",
    "\n",
    "H, status = img_align.get_homography(img2, img1)\n",
    "#apply the homography for overlapping images, meanwhile performing blending between\n",
    "#them\n",
    "panorama_image = images_blending(img1, img2, width_panorama, height_panorama, H)\n",
    "\n",
    "#saving the panorama image\n",
    "#cv2.imwrite(os.path.join(fn_base,\"panorama.jpg\"), panorama_image)\n",
    "plt.imshow(panorama_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. DO IT YOURSELF!\n",
    "---\n",
    "Go through the code again and change the parameters given to the constructor of the images_alignment class, so that you can implement the image stitching pipeline with SURF and ORB features. Compare the results obtained in terms of speed and accuracy.\n",
    "Which one is the most accurated method? Which one the least? Which one is the faster method? Which one is the slower?\n",
    "I hope you have enjoyed it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. ANNEX\n",
    "---\n",
    "A. Visualize detected features and paired matches\n",
    "Look at the snippet of code below. It is really important to visualize the outcome for debugging purposes. It may help your understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def explore_match (img1 , img2 , kp_pairs , status = None , H = None):\n",
    "    h1, w1 = img1.shape[:2]\n",
    "    h2, w2 = img2.shape[:2]\n",
    "    vis = np.zeros((max(h1, h2), w1+w2), np.uint8) \n",
    "    vis[:h1, :w1] = img1\n",
    "    vis[:h2, w1:w1+w2] = img2\n",
    "    vis = cv2.cvtColor ( vis , cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    if H is not None:\n",
    "        corners = np.float32([[0, 0], [w1, 0], [w1, h1], [0, h1]])\n",
    "        corners = np.int32( cv2.perspectiveTransform(corners.reshape(1, -1, 2), H).reshape(-1, 2) + (w1, 0) ) \n",
    "        cv2.polylines(vis , [corners], True, (255, 255, 255))\n",
    "        \n",
    "    if status is None:\n",
    "        status = np.ones(len(kp_pairs), np.bool )\n",
    "    p1 = np.int32([kpp[0].pt for kpp in kp_pairs])\n",
    "    p2 = np.int32([kpp[1].pt for kpp in kp_pairs]) + (w1, 0) #w1 is the width of the first image\n",
    "    \n",
    "    green = (0, 255, 0)\n",
    "    red = (0, 0, 255)\n",
    "    white = (255 , 255 , 255)\n",
    "    kp_color = (51, 103, 236)\n",
    "    for (x1, y1), (x2, y2), inlier in zip(p1, p2, status):\n",
    "        if inlier:\n",
    "            col = green\n",
    "            cv2.circle(vis, (x1, y1), 2, col, -1) \n",
    "            cv2.circle(vis, (x2, y2), 2, col, -1)\n",
    "        else:\n",
    "            col = red\n",
    "            r=2\n",
    "            thickness = 3\n",
    "            cv2.line(vis, (x1-r, y1-r), (x1+r, y1+r), col, thickness)\n",
    "            cv2.line(vis, (x1-r, y1+r), (x1+r, y1-r), col, thickness)\n",
    "            cv2.line(vis, (x2-r, y2-r), (x2+r, y2+r), col, thickness)\n",
    "            cv2.line(vis, (x2-r, y2+r), (x2+r, y2-r), col, thickness)\n",
    "    \n",
    "    vis0 = vis.copy()\n",
    "    for (x1, y1), (x2, y2), inlier in zip(p1, p2, status):\n",
    "        if inlier:\n",
    "            cv2.line(vis, (x1, y1), (x2, y2), green)\n",
    "    return vis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
